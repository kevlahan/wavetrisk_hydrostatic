1) Removing patches that are not the two locally finest levels,
   with the goal to better distribute patches for highly localized test-cases:

 - call remove_inside_patches(), for now after initializing and after loading checkpoint
 - uncomment the two lines in src/domain.f90 that are marked with "TODO{uncomment & test}"
 - implement communication of patches
 - add three communication steps in remove_inside_patches() each communicating patches
    1. communicate Patch%active after patch_count_active has been evaluated
    2. save result of check_children_fillup in patches and communicate it afterwards
    3. communicate for every parent patch whether (children_fullness .gt. FILLUP_THRESHOLD), i.e. whether it will be removed
   set_masks with FROZEN argument must be called after 3rd comm
 - change get_child_and_neigh_patches and its callers to use the communicated information from boundary patches
 - for J_min unequal J_max and eps=0, check that children_fullness about equal 1

At this point check that everything is working before continuing, e.g.
  * run some test-cases and check that results look visually the same
  * if possible visualize the remove_inside_patches routine, e.g. by colouring cells according to the value of `active` of their patch

Other changes required:
  + bdry patch can no longer get the neighbour domain from their own domain since there might be several neighbours on one side
    -> each bdry patch must know the neighbour as a <domain ID, patch ID> pair


2) Support write-out netCDF format (summary: easily with Matlab for non-adaptive grids, tricky for adapted grids)
    possibly advisable to write Python or Matlab script to easily convert current output to netCDF
    rather then struggling with MPI and Fortran-netCDF
   either way, question:
    will netCDF based post-processing tools accept adaptive grids?
    (triangular grid maybe if we write out only locally finest elements
     [edit: triangular definitely if we save triangles side at coarse/fine transition with coarse edge bisected (-> quadrangle)],
     polygons with complicated overlap probably not?)
   strategy for writing adapted grid as unstructured triangle/quadrangle
    - write triangle if all three vertices are active (.ge. ADJZONE) and no edge bisections on finer level is active
    - if additionally one edge bisection is active write as quadrangle with this edge bisected
    - if two bisections are active write as quadrangle with the two midpoints and the two vertices that are connected by the not bisected edge
    - if all this bisected skip
    this should produce a correct connectivity/topology

3) avoid bathymetry smoothing after each checkpoint and loss of mass conservation in one go:
   - create a J_min=x J_max=12 grid
   - compute bathymetry on level 12 using any smoothing technique desired
   - compute the corresponding wavelet coefficients and J_min scaling coefficients
   - store them in a file which is in future used instead of the lon-lat bathymetry data
   What to do about the penalization parameter?
   - would be good to be consistent with interpolation of penalized height

4) load-balance with MPI instead through disk
   -> force rebalancing if on one cpu load grows so big, that we risk running out of memory
   - otherwise rebalance when imbalance is big enough 
     -> need a way to check if better balance is even possible, before rebalancing
        idea: write out load (file "conn.*") like now and try compute load distribution like now (using temporary variables)
              and compare new imbalance to current imbalance
              also compare max. load, if it can even be reduced
        ! this could also be used when rebalancing through disk !!
   as a first step:
     could try to first MPI-communicate contents of conn.*
     then everyone can compute the same distribution, like after restart
     then, in the exact same way as currently to file, write to array in memory instead
     now send and receive all arrays that are on wrong cpu
     now,  in the exact same way as currently from file, read from array in memory instead
   !! this first step could also be used to avoid having open too many files the same time !!
      (additionally subdirectorys can be created to avoid having 10000 files in one directory)

