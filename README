******************************************************************************************************************************************************************
A brief user's guide for WAVETRISK 1.0
2019-04-01 N. Kevlahan kevlahan@mcmaster.ca
******************************************************************************************************************************************************************
TEST CASES:
Test cases are defined in separarate subdirectories of test/ (e.g. test/Held_Suarez).  Each test case consists of three files:

- the main program (e.g. Held_Suarez.f90)
- test_case_module.f90 (which sets initial conditions, defines the vertical grid, defines and reads/prints parameters for this test case)
- test_case.in (a file setting the parameters for the run)


******************************************************************************************************************************************************************
COMPILING and RUNNING 

TO COMPILE (using the test case Held_Suarez as an example):
mkdir build
make TEST_CASE=Held_Suarez PARAM=param_J5 ARCH=mpi-lb

Selects Held_Suarez test case, J=5 coarsest grid and load balancing parallel (mpi) version. Can use "make clean" to ensure a clean build. Can also compile as
serial (ARCH=ser) or without load balancing (ARCH=mpi).  The code has been tests for the gfortran and ifort compilers and the openmpi and intelmpi mpi libraries.

!! Always do "make clean" when compiling a new test case !!

Note that the code requires the LAPACK library and an appropriate link must be defined in Makefile.

TO RUN (using test case Held_Suarez as an example):
cd to-working_directory                                  (move to working directory, mkdir if necessary)
ln -s wavetrisk_hydrostatic/bin/Held_Suarez .            (provide symbolic link to executable)
ln -s path-to-grid_HR .                                  (provide symbolic link to Heikes and Randall grids, if using)
cp wavetrisk_hydrostatic/test/Held_Suarez/test_case.in . (copy parameters files and then edit as appropriate)
mpirun -n 40 Held_Suarez > log &                         (run on 40 processors using mpi)


******************************************************************************************************************************************************************
POST-PROCESSING DATA

POST-PROCESS SAVED FIELDS (e.g. Held_Suarez.1... and Held_Suarez.2...) FOR PARAVIEW:

cd post
gfortran -o trisk2vtk trisk2vtk.f90
cd to-working-directory
ln -s wavetrisk_hydrostatic/post/trisk2vtk .
trisk2vtk Held_Suarez.1 primal 0 10 5 8 out (generates .vtk files for the saved primal grid fields 0 to 10 at scales j = 5:8 and saves in files out...vtk)
trisk2vtk Held_Suarez.2 dual   0 10 5 8 out (generates .vtk files for the saved primal grid fields 0 to 10 at scales j = 5:8 and saves in files out...vtk)


POST-PROCESS CHECKPOINTS FOR MATLAB PLOTTING OF 2-D PROJECTIONS ON LONGITUDE-LATITUDE PLANE:

The m-file plot_2d.m provides various options for plotting 2-D projections of the data and of zonal statistics. The zonal statistics can be plotted from in-line
calculations (e.g. Held_Suarez_J4L18.3.tgz file saved during the run) or from a data post-processed from saved checkpoints (e.g. Held_Suarez_J4L18.4.tgz)
using the test case flat_projection_data.  plot_2d.m also plots time history data from the log file saved during the run (e.g. Held_Suarez_log).


******************************************************************************************************************************************************************
BASICS OF THE CODE


GRID STRUCTURE 

The icosahedron is divided into a network of ten regular lozenge grids (with the exception of the POLES), each lozenge is then divided
into N_SUB_DOM = 4**DOMAIN_LEVEL regular sub-domains with the number of sub-domains on each processor given by n_domain(rank+1).

There are two special nodes called POLEs. One connects the five lozenge vertices at the top of the network and the other connects the 
five lozenge grid vertices at the bottom of the network. The network is oriented so the POLEs are at the geographic North and South poles.  

The coarsest level Jmin = DOMAIN_LEVEL + PATCH_LEVEL + 1 with PATCH_LEVEL>=2. The geometry of this coarsest level may be optimized by reading 
in a Heikes & Randall (1995) grid of resolution Jmin-1 or using the Xu (2006) smoothing algorithm.
The size of patches is 2**PATCH_LEVEL (e.g. if PATCH_LEVEL = 2 then the patches are 4x4).
The larger PATCH_LEVEL is the fewer levels of tree structure must be traversed before reaching a uniform grid (which will in general contain inactive nodes).
Each computation patch element is made up of one node for scalars and three edges for vectors U, V,W (see also GRID ELEMENTS below).

PARALLEL EXECUTION

If compiled with ARCH = mpi or mpi-lb (load balancing) the code can be run in parallel using mpi.  With option mpi-lb the computational load is re-balanced
each time a checkpoint is saved.
 
The maximum number of computational cores must be less than or equal to the number of domains, i.e. Ncore <= 10*4**DOMAIN_LEVEL.
Since DOMAIN_LEVEL = Jmin - (PATCH_LEVEL+1) larger Jmin or smaller PATCH_LEVEL allows more cores to be used.  For inhomogeneous problems
(i.e. unbalanced) it is best to set PATCH_LEVEL=2 and use a larger Jmin while for homogeneous (i.e. well-balanced) problems it is more efficient to choose
a larger PATCH_LEVEL. For example, if Jmin = 7 and PATCH_LEVEL = 4 then up to 160 cores may be used, while if PATCH_LEVEL = 2 then up to 2560 cores may be used.

BASIC GRID DATA TYPES

Type(Coord_Array) has components elts(:)%length (where elts is Type(Coord) array of x%y%z coordinates on the sphere for each grid element and
length is an integer giving the size of elts).

Type(Float_Array) has components elts(:)%length (where elts is a real array of variable values on each grid element on each sub-domain and
and length is an integer giving the size of elts). It is used for physical variables such as coriolis, divu, vort etc.

Type(Int_Array) has components elts(:)%length (where elts is an integer array of element indices on each sub-domain and
and length is an integer giving the size of elts). It is used for masks, levels and parallel communication variables.

Type(Float_Field) has components data(:)%bdry_uptodate%pos (where data is a Float_Array, bdry_uptodate is a logical and pos is an integer)
and is used for equation variables such as sol, trend, wav_coeff, sca_coeff.

Type(Domain) has many components defining the grid and is the size of the total number of sub-domains on each processor n_domain(rank+1).
It is used for the variable grid(:).

Various other dynamical data types and subroutines for allocating them are defined in dyn_array.f90.


GRID ELEMENTS

The triangular primal grid element (one hexagonal node H, three edges UP, DG, RT and two triangular nodes UPLT, LORT) is

             ------------ 
             \           / \ 
              \  UPLT   /   \ 
               \       /     \
               UP     DG      \   
                 \   /   LORT  \
                  \ /           \
                   H ------RT---- 


The hexagonal dual grid element: one hexagonal node H, three adjacent edges UP, DG, RT and two triangular nodes O
(rotated clockwise 30 degrees wrt primal grid above) is



             -----UP---- UPLT 
           /               \ 
          /                 \ 
         /                  DG
        /                     \   
       /                       \
                   H           LORT
       \                       /
        \                     /
         \                   RT
          \                 / 
           \               /
             -------------


Patch neighour directions (based on regular coordinates i,j).
Note that within a patch similar notation is used for shifts in i and j (e.g. shift i-1, j+1 is denoted idNW).
                   
 ------------- ------------- ------------- 
\              \             \            \
 \              \             \            \
  \              \             \            \
   \  NORTHWEST   \    NORTH    \ NORTHEAST  \  
    \              \             \            \
     \              \             \            \
       -------------  ------------- ------------- 
       \              \             \            \
        \              \             \            \
         \              \             \            \  
          \     WEST     \      0      \    EAST    \ 
           \              \             \            \
            \              \             \            \
              -------------  ------------- ------------- 
              \              \             \            \
               \              \             \            \ 
                \              \             \            \
                 \   SOUTHWEST  \  SOUTH      \  SOUTHEAST \
                  \              \             \            \
                   \              \             \            \
                     -------------   ------------ ------------ 




INDEXING OF GRID ELEMENTS AND NEIGHBOURS

Quantities (e.g. mass, velocities) are all stored in a single array, whose elements are organized in patches.
Each patch has regular array coordinates (i,j).

Patch offset array offs(N_BDRY+1) contains the starting index in the single array for the current patch as offs(0). The starting 
indices for neighbouring patches are given by offs(NORTH) etc.

Patch dimension array dims(2, N_BDRY+1) gives the dimensions of the current patch as dims(2,0) and neighbouring patches as dims(2, NORTH) etc.

Function id = idx(i,j,offs,dims) returns the element index for the hexagon node with coordinates (i,j) on the patch selected by offs with dimensions dim. 

The components of the grid elements are then found as:

 %elts(id+1)         - the one grid element hexagon node (e.g. masses)
 %elts(EDGE*id+e+1)  - the three grid element edges RT, DG, UP, where e = 0,1,2 (e.g. velocities, fluxes)
 %elts(TRIAG*id+t+1) - the two grid element triangles LORT, UPLT, where t = 0,1 (e.g. circulation)

Wavelet coefficients are stored at the SAME nodes/edges as the nodes/edges they characterize.

DATA OUTPUT

Data is written for plotting by routines io.f90/write_primal and io.f90/write_dual. 
The full state of the simulation is saved in io.f90/dump_adapt_mpi and read in again by io.f90/load_adapt_mpi.

CALCULATIONS ON ADAPTED GRID

By default fields are calculated and operators are applied on the ENTIRE grid (including at nodes where the result can be obtained by  restriction 
indicated by mask=12 and adjacent zone nodes indicated by mask=8) and the results are then over-written by correct values.  
(Note that the solution in the adjacent zone at fine scale j+1 are found from values at coarse scale j so the values calculated at scale j+1 are not
actually used.)

This means that some operations on the entire grid could produce intermediate overflows, inf/NaN, or invalid indices ue to incorrect values at these nodes or
ther neighbours. Functions and subroutines should take this into account.  Similarly, circulation, vorticity and qe are first computed incorrectly
at pentagon points in step1 and then corrected in post_step1.



