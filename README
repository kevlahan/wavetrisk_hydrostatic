**********************************************************************************************************************************
A brief user's guide for WAVETRISK 2.2 2021-10-28
N Kevlahan
Department of Mathematics and Statistics
McMaster University
Hamilton, Canada
kevlahan@mcmaster.ca

A dynamically adaptive wavelet-based experimental global climate model that solves the multi-layer hydrostatic rotating shallow  
water equations in Boussinesq form using a mimetic method.  Both compressible (atmosphere) and incompressible (ocean) options are
available. 

**********************************************************************************************************************************
Major changes compared to WAVETRISK 2.1
1. Addition of dynamic load balancing using charm++/AMPI.

Major changes compared to WAVETRISK 1.x
1. Addition of spherical harmonics test case and associated matlab m-file for computing energy spectra.
2. Additional ocean modelling test cases.
3. Addition of vertical diffusion for ocean modelling using a TKE closure similar to that used in NEMO.
4. Option of implicit time step for horizontal diffusion.
5. Option of semi-implicit barotropic-baroclinic time integration.
6. Adaptive multigrid solver, using scheduled relaxation Jacobi (SRJ) iterations.

**********************************************************************************************************************************
TEST CASES

Test cases are defined in separate subdirectories of test/ (e.g. test/Held_Suarez).  Each test case consists of three files:

- the main program (e.g. Held_Suarez.f90) test_case_module.f90 (which sets initial conditions, defines the vertical grid, defines
- and reads/prints parameters for this test case) test_case.in (a file setting the parameters for the run)


**********************************************************************************************************************************
COMPILING and RUNNING

If compiled with ARCH = mpi or mpi-lb (load balancing) the code can be run in parallel using mpi.  ARCH=ser compiles a serial
version, primarily for testing. With option mpi-lb the computational load is re-balanced at each checkpoint using a simple
next fit algorithm .

LOAD BALANCING
If the code is compiled with MPIF90=ampi (instead of MPIF90=mpi) the load is checked and rebalanced if necessary every irebalance 
time steps using charm++/AMPI. You need to first install the charm++/AMPI library (example for version 7.0 where pieglobals are
not yet included by default). The following gives an interactive build (you should choose gcc and gfortran compilers):

git clone https://github.com/UIUC-PPL/charm
cd charm/
git fetch  &&  git checkout pieglobals-testing  
./build

We have found that the following works to build AMPI with intel compilers (using the Compute Canada machine niagara as an
example):

module load NiaEnv/.2021a cmake intel intelmpi ucx

CMK_NATIVE_CXX=icpc CMK_NATIVE_LDXX=icpc ./build AMPI ucx-linux-x86_64 icc ifort --with-production  -c++ icpc

COMPILING 
The maximum number of computational cores must be less than or equal to the number of domains, i.e. Ncore <= 10*4**DOMAIN_LEVEL.
Since DOMAIN_LEVEL = Jmin - (PATCH_LEVEL+1) larger Jmin or smaller PATCH_LEVEL allows more cores to be used.  For inhomogeneous
problems (i.e. unbalanced) it is best to set PATCH_LEVEL=2 and use a larger Jmin while for homogeneous (i.e. well-balanced)
problems it is more efficient to choose a larger PATCH_LEVEL. For example, if Jmin = 7 and PATCH_LEVEL = 4 then up to 160 cores
may be used, while if PATCH_LEVEL = 2 then up to 2560 cores may be used.

TO COMPILE
Using the test case Held_Suarez as an example:

mkdir build
make TEST_CASE=Held_Suarez PARAM=param_J5 ARCH=mpi-lb

or to compile test case jet using charm++/AMPI:

make build
make TEST_CASE=jet PARAM=param_J6 MPIF90=ampi

Selects Held_Suarez test case, J=5 coarsest grid and load balancing parallel (mpi) version. Can use "make clean" to ensure a clean
build. Can also compile as serial (ARCH=ser) or without load balancing (ARCH=mpi).  The code has been tested for the gfortran and
ifort compilers and the openmpi and intelmpi mpi libraries.

!! Always do "make clean" when compiling a new test case and when you have modified the main program (e.g. jet.f90)!!

RUNNING
(using test case Held_Suarez as an example): cd to-working_directory (move to working directory, mkdir if necessary) ln -s
wavetrisk_hydrostatic/bin/Held_Suarez .  (provide symbolic link to executable) ln -s path-to-grid_HR .  (provide symbolic link to
Heikes and Randall grids, if using) cp wavetrisk_hydrostatic/test/Held_Suarez/test_case.in . (copy parameters files and then edit
as appropriate) g & (run on 40 processors using mpi)

The code often runs significantly faster with mpirun than with srun when using the slurm scheduler.

If running with bathymetry data (e.g. incompressible tsunami test case) need to provide a symbolic link to the ETOPO data:

ln -s ../extern/bathymetry/2arcminutes.smoothed bathymetry (to use 2 arcminute etopo topography data)

Examples:
1. To run test case Held_Suarez with input file HS.in with mpirun on 40 cores

mpirun -n 40 Held_Suarez HS.in

2. To run test case jet with default input file test_case.in with charm++/AMPI on 60 cores with 160 virtual processors
using "Refine load balancing" and outputting details of the load balancing:

/charmrun +p 60 ./jet_J6 +vp 160 +balancer RefineLB +LBDebug 1 > log&


**********************************************************************************************************************************
POST-PROCESSING DATA USING PARAVIEW AND MATLAB

The raw data files (e.g. Held_Suarez.1... and Held_Suarez.2...) must be converted to .vtk format for viewing with paraview:
 
cd post gfortran -o trisk2vtk trisk2vtk.f90 cd to-working-directory ln -s wavetrisk_hydrostatic/post/trisk2vtk .  trisk2vtk
Held_Suarez.1 primal 0 10 5 8 out (generates .vtk files for the saved hexagon grid fields 0 to 10 at scales j = 5:8 and saves in
files out...vtk) trisk2vtk Held_Suarez.2 dual 0 10 5 8 out (generates .vtk files for the saved triangle grid fields 0 to 10 at
scales j = 5:8 and saves in files out...vtk)


POST-PROCESS CHECKPOINTS FOR MATLAB PLOTTING OF 2-D PROJECTIONS ON LONGITUDE-LATITUDE PLANE:

The m-file plot_2d.m provides various options for plotting 2-D projections of the data, zonal statistics and time history
statistics.  The zonal statistics can be plotted from in-line calculations (e.g. Held_Suarez_J4L18.3.tgz file saved during the
run) or from data post-processed from saved checkpoints (e.g. Held_Suarez_J4L18.4.tgz) using test/flat_projection_data.  plot_2d.m
plots time history data from the log file saved during the run (e.g. Held_Suarez_log).  (See also 2D PROJECTION below.)
Additional post-processing m-files are provided for specific test cases and applications.


SPHERICAL HARMONICS GLOBAL AND LOCAL SPECTRAL ANALYSIS

The test case spherical_harmonics provides tools for computing spherical harmonics energy spectra. The energy spectra may be
computed over the entire sphere, or over a local spherical cap region.  This test case uses the SHTOOLS package, which is
available at https://shtools.github.io/SHTOOLS/index.html.  The algorithms used in SHTOOLS are described in:

Wieczorek, M. A. and F. J. Simons 2007 Minimum-variance multitaper spectral estimation on the sphere, J. Fourier Anal. Appl., 13,
doi:10.1007/s00041-006-6904-1, 665-692.

Wieczorek, M. A. and Meschede, M. 2018 SHTools: Tools for Working with Spherical Harmonics.Geochemistry, Geophysics, Geosystems,
19(8), 2574-2592.

SHTOOLS requires the libraries lapack, blas and fftw3.

An example matlab m-file spherical_harmonic_analysis.m is also provided for visualizing the spectra produced by this test case.


**********************************************************************************************************************************
BASICS OF THE CODE

PROGNOSTIC VARIABLES
The variables are stored in a Float_Array (see below) sol, with components S_MASS, S_TEMP and S_VELO.  The code solves the
hydrostatic Boussinesq equations in compressible (i.e. atmosphere) or incompressible (i.e. ocean) form.

S_MASS contains mu = ref_density * dz (i.e. kinemetic/inert pseudo density).

S_TEMP contains the mass-weighted potential temperature mu * potential temperature in the compressible case or mu *
density/ref_density = density * dz (i.e. gravitational mass) in the incompressible case. The variable theta is defined to be
S_TEMP/S_MASS, so theta is the potential temperature in the compressible case, and theta is the normalized total density in the
incompressible case (i.e. buoyancy = grav_accel * (1 - theta)).

S_VELO contains the three velocity components U, V, W at edges RT, DG and UP respectively (see below).


HORIZONTAL COORDINATES
Cartesian (x,y,z) coordinates are used with spherical geometry for lengths and areas on the sphere.  Since the equations are
written in vector invariant form, there are no explicit metric terms.

VERTICAL COORDINATES
The zero level of the vertical coordinate is at mean sea level and vertical levels are always indexed starting at the lowest
level.  The coordinate of the local topography is given by dom%topo%elts(id_i) (or by the test case function surf_geopot in the
case of analytical topography), which is positive for orography and negative for bathymetry.  For incompressible (i.e. ocean)
simulations the mean depth is defined to be negative, H0 = dom%topo%elts(id_i) < 0, and positive perturbations to the free surface
are positive.  The local total depth is H = sum_zlev ( sol(S_MASS,zlev)%data(d)%elts(id_i) ) / ref_density and the free surface
perturbation is therefore H + H0.

The vertical coordinates are Lagrangian (i.e. they move as material surfaces) with periodic remapping onto a target (e.g. initial
grid).  A collection of different conservative piecewise remapping routines are available.

SOLID BOUNDARIES
In ocean test cases solid lateral boundaries are modelled using Brinkman volume penalization (see Kevlahan, Dubos & Aechtner 2015,
Geosci Model Dev 8).  The permeability parameter is fixed equal to dt and the porosity parameter alpha is set by the user
(e.g. alpha = 0.01). The permeability friction term -1/eta u in the velocity equation is applied as a separate split step after the
main time step, as in Rassmussen, Cottet & Walther (J Comput Phys 230, 2011).

BAROTROPIC - BARCOCLINIC MODE SPLITTING
Incompressible (i.e. ocean) simulations can be run with 2D barotropic - 3D baroclinic mode splitting to avoid the stability
constraint of the fast external mode with speed c = sqrt(g H). Setting mode_split = .true. chooses mode splitting, which integrates
the barotropic component implicitly in time on the slow geostrophic time scale. The computation is stable with
CFL_barotropic = sqrt(g H) dt/dx < = 30, and often at larger values. The method is similar to the "implicit free surface"
option in MITgcm.  The splitting usese a semi-implicit theta-method for the barotropic mode, with parameters theta1 (for the
external pressure gradient) and theta2 for the barotropic flow divergence.  Theta1 = theta2 = 1 gives a backwards Euler (fully
implicit scheme), and theta1 > 0.5, theta2 > 0.5 is necessary for stability.  The recommended value is thata1 = theta2 = 0.8.

The elliptic equation for the free surface is solved using a simple adaptive multigrid solver. The coarsest scale is solved using bicgstab,
and this solution is then prolonged to the finer scales, which are solved using scheduled relaxation Jacobi iterations (Adsuara et al
J Comput Phys 332, 2017). Recommended relative tolerance for the (coarse scale) elliptic solver tol_elliptic = 1e-9 to ensure mass conservation and
sufficient accuracy. A larger relative tolerance tol_jacobi = 1e-3 to 1e-6 may be used for the finer scales. 

The associated elliptic equation is solved using a simple multigrid scheme on the adaptive grid. The baroclinic and barotropic
modes are coupled at each time step. The barotropic part computes an implicit free surface. The barotropic estimate of the location of
the free surface is reconciled with the baroclinic estimate using layer dilation (Bleck and Smith 1990, J Geophys Res 95, 3273-3285).
Layer dilation means that mass is not exactly conserved in each vertical layer, although it is conserved over all vertical layers.
Mode splitting is suitable when the user is not interested in the evolution of the free surface, since it diffuses free
surface motion. When the option remap = .false. it is assumed that the density remains constant in each vertical layer, equal to the
initial density distribution.  Otherwise, horizontal density gradients are included with Ripa dynamics (e.g. Ripa 1993, Geophys Astrophys Fluid, 70).
The drake test case also includes an option that relaxes the layer densities to their initial values. This model is also called IL0 (Beron-Vera 2021,
Rev Mex Fis 67), or a "thermal rotating shallow-water model".

The parameter level_fill can be set > min_level to make active all levels <= level_fill. This sometimes improves the efficiency of the
multigrid solver by reducing the size of of the coarsest grid where the elliptic equation is solved exactly.


VERTICAL DIFFUSION AND SURFACE FLUXES IN OCEAN MODEL (INCOMPRESSIBLE CASE)
Eddy diffusion of buoyancy and eddy viscosity for velocity is implemented using a flux-based implicit time integration scheme in the module
vert_diffusion_mod (option vert_diffuse = .true.).  Wind stress, bottom friction and buoyancy fluxes are included via flux (Neumann) boundary conditions.
A solar radiation model (as in NEMO) is activated by setting a non-zero value for the solar radiation flux Q_sr .  Two options are available:
tke_diffuse = .true. uses a turbulent kinetic energy (TKE) closure scheme, similar to that in NEMO, tke_diffuse = .false. uses a simple depth-based
eddy-viscosity.


GRID STRUCTURE
The icosahedron is divided into a network of ten regular lozenge grids (with the exception of the POLES), each lozenge is then
divided into N_SUB_DOM = 4**DOMAIN_LEVEL regular sub-domains with the number of sub-domains on each processor given by
n_domain(rank+1).

There are two special nodes called POLEs. One connects the five lozenge vertices at the top of the network and the other connects
the five lozenge grid vertices at the bottom of the network. The network is oriented so the POLEs are at the geographic North and
South poles.

The coarsest level Jmin = DOMAIN_LEVEL + PATCH_LEVEL + 1 with PATCH_LEVEL>=2. The geometry of this coarsest level may be optimized
by reading in a Heikes & Randall (1995) (optimize_grid = HR_GRID) grid of resolution Jmin-1 or using the Xu (2006) smoothing algorithm
(optimize_grid = XU_GRID). If optmize_grid = NONE the coarsest grid is just the grid produced by bisecting the icosahedron the Jmin times.
The size of patches is 2**PATCH_LEVEL (e.g. if PATCH_LEVEL = 2 then the patches are 4x4).  The larger PATCH_LEVEL is the fewer levels of tree
structure must be traversed before reaching a uniform grid (which will in general contain inactive nodes).  Each computation patch
element is made up of one node for scalars and three edges for vectors U, V,W (see also GRID ELEMENTS below).


2D PROJECTION ONTO A LONGITUDE-LATITUDE GRID
The module projection_mod includes routines for projecting data from the sphere to a uniform longitude-latitude grid.  These can be used to
save projections for post-processing, spherical harmonics, or for zonal averaging.  Note that these routines require a "full" (i.e. non-adaptive
grid) which can be either min_lev, or computed using call fill_up_grid_and_IWT (level_save).


BASIC GRID DATA TYPES
Type(Coord_Array) has components elts(:)%length (where elts is Type(Coord) array of x%y%z coordinates on the sphere for each grid
element and length is an integer giving the size of elts).

Type(Float_Array) has components elts(:)%length (where elts is a real array of variable values on each grid element on each
sub-domain and and length is an integer giving the size of elts). It is used for physical variables such as coriolis, divu, vort
etc.

Type(Int_Array) has components elts(:)%length (where elts is an integer array of element indices on each sub-domain and and length
is an integer giving the size of elts). It is used for masks, levels and parallel communication variables.

Type(Float_Field) has components data(:)%bdry_uptodate%pos (where data is a Float_Array, bdry_uptodate is a logical and pos is an
integer) and is used for equation variables such as sol, trend, wav_coeff, sca_coeff.

Type(Domain) has many components defining the grid and is the size of the total number of sub-domains on each processor
n_domain(rank+1).  It is used for the variable grid(:).

Various other dynamical data types and subroutines for allocating them are defined in dyn_array.f90.


GRID ELEMENTS
The triangular primal grid element (one hexagonal node H, three edges UP, DG, RT and two triangular nodes UPLT, LORT) is

             ------------ 
             \           / \ 
              \  UPLT   /   \ 
               \       /     \
               UP     DG      \   
                 \   /   LORT  \
                  \ /           \
                   H ------RT---- 


The hexagonal dual grid element: one hexagonal node H, three adjacent edges UP, DG, RT and two triangular nodes O
(rotated clockwise 30 degrees wrt primal grid above) is



             -----UP---- UPLT 
           /               \ 
          /                 \ 
         /                  DG
        /                     \   
       /                       \
                   H           LORT
       \                       /
        \                     /
         \                   RT
          \                 / 
           \               /
             -------------


Patch neighour directions (based on regular coordinates i,j).
Note that within a patch similar notation is used for shifts in i and j (e.g. shift i-1, j+1 is denoted idNW).
                   
 ------------- ------------- ------------- 
\              \             \            \
 \              \             \            \
  \              \             \            \
   \  NORTHWEST   \    NORTH    \ NORTHEAST  \  
    \              \             \            \
     \              \             \            \
       -------------  ------------- ------------- 
       \              \             \            \
        \              \             \            \
         \              \             \            \  
          \     WEST     \      0      \    EAST    \ 
           \              \             \            \
            \              \             \            \
              -------------  ------------- ------------- 
              \              \             \            \
               \              \             \            \ 
                \              \             \            \
                 \   SOUTHWEST  \  SOUTH      \  SOUTHEAST \
                  \              \             \            \
                   \              \             \            \
                     -------------   ------------ ------------ 


INDEXING OF GRID ELEMENTS AND NEIGHBOURS
Quantities (e.g. mass, velocities) are all stored in a single array, whose elements are organized in patches.  Each patch has
regular array coordinates (i,j).

Patch offset array offs(N_BDRY+1) contains the starting index in the single array for the current patch as offs(0). The starting
indices for neighbouring patches are given by offs(NORTH) etc.

Patch dimension array dims(2, N_BDRY+1) gives the dimensions of the current patch as dims(2,0) and neighbouring patches as dims(2,
NORTH) etc.

Function id = idx(i,j,offs,dims) returns the element index for the hexagon node with coordinates (i,j) on the patch selected by
offs with dimensions dim.

The components of the grid elements are then found as:

 %elts(id+1) - the one grid element hexagon node (e.g. masses) elts(EDGE*id+e+1) - the three grid element edges RT, DG, UP, where
 %e = 0,1,2 (e.g. velocities, fluxes) elts(TRIAG*id+t+1) - the two grid element triangles LORT, UPLT, where t = 0,1
 %(e.g. circulation)

Wavelet coefficients are stored at the SAME nodes/edges as the nodes/edges they characterize.

DATA OUTPUT
Data is written for plotting by routines io.f90/write_primal and io.f90/write_dual.  The full state of the simulation is saved in
io.f90/dump_adapt_mpi and read in again by io.f90/load_adapt_mpi.

CALCULATIONS ON ADAPTED GRID
By default fields are calculated and operators are applied on the ENTIRE grid (including at nodes where the result can be obtained
by restriction indicated by mask=12 and adjacent zone nodes indicated by mask=8) and the results are then over-written by correct
values.  (Note that the solution in the adjacent zone at fine scale j+1 are found from values at coarse scale j so the values
calculated at scale j+1 are not actually used.)

This means that some operations on the entire grid could produce intermediate overflows, inf/NaN, or invalid indices due to
incorrect values at these nodes or their neighbours. Functions and subroutines should take this into account.  Similarly,
circulation, vorticity and qe (potential vorticity are first computed incorrectly at pentagon points in step1 (or cal_vort) and
then corrected in post_step1 (or post_vort).
