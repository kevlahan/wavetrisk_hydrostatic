**********************************************************************************************************************************
*   WAVETRISK 2.2														 *
*   2022-01-20															 *
*																 *
*   User's guide and code description												 *
*															         *
*   N Kevlahan															 *
*   Department of Mathematics and Statistics											 *
*   McMaster University	    													 *
*   Hamilton, Canada														 *
*   kevlahan@mcmaster.ca													 *
* 																 *
*   WAVETRISK: A dynamically adaptive wavelet-based experimental global climate model that solves the multi-layer hydrostatic    *
*   rotating shallow water equations in Boussinesq form using a mimetic method. Compressible (atmosphere) and incompressible 	 * 
*   (ocean) options are available by setting the parameter compressible = .false. or .true. respectively.	   		 *
*															         *
*																 *
**********************************************************************************************************************************
*																 *
*   Major changes compared to WAVETRISK 2.1											 *
*   1. Addition of dynamic load balancing and checkpointing using charm++/AMPI.							 *
*																 *
*   Major changes compared to WAVETRISK 1.x											 *
*   1. Addition of spherical harmonics test case and associated matlab m-file for computing energy spectra.			 *
*   2. Additional ocean modelling test cases.    	 	    	   	      		       				 *
*   3. Addition of vertical diffusion for ocean modelling using a TKE closure similar to that used in NEMO.			 *
*   4. Option of implicit time step for horizontal diffusion.									 *
*   5. Option of semi-implicit barotropic-baroclinic time integration.								 *
*   6. Adaptive multigrid solver, using scheduled relaxation Jacobi (SRJ) iterations.						 *
*   	    	      	      	    	      		 	  								 *
**********************************************************************************************************************************
* References:															 *
*																 *
* Kevlahan, N.K.-R. 2021 Adaptive wavelet methods for Earth systems modelling.  Fluids 6 doi.org/10.3390/ fluids6070236.     	 *
*																 *
* Kevlahan, N.K.R. & Dubos, T. 2019 WAVETRISK-1.0: an adaptive wavelet hydrostatic dynamical core. Geosci. Model Dev. 12,	 *
* 4901â€“4921. doi.org/10.5194/gmd-12-4901-2019.	      	       	       		   	     	   	   	      		 *
*																 *
* Kevlahan, N.K.-R., Dubos, T. Aechtner, M. 2015 Adaptive wavelet simulation of global ocean dynamics using a new Brinkman	 *
* volume penalization. Geosci. Model Dev. 8 3891-3909.  doi:10.5194/gmd-8-3891-2015.   	     	      	      	  		 *
*																 *
* Aechtner, M. Kevlahan, N.K-R. & Dubos, T. 2015 A conservative adaptive wavelet method for the shallow-water equations on	 *
* the sphere. Q. J. R. Meteorol. Soc. 141(690), 1712-1726 doi:10.1002/qj.2473.	 	    		      			 *
*																 *
* Dubos, T. & Kevlahan, N.K.-R. 2013 A conservative adaptive wavelet method for the shallow-water equations on staggered grids.	 *
* Q. J. R. Meteorol. Soc. 139, 1997-2020 doi:10.1002/qj.2097.									 *
**********************************************************************************************************************************


**********************************************************************************************************************************
USER'S GUIDE


**********************************************************************************************************************************
TEST CASES

Test cases are defined in separate subdirectories of test/ (e.g. test/Held_Suarez).  Each test case consists of three files:

1. The main program (e.g. Held_Suarez.f90).
2. A file setting various case-specific properties: test_case_module.f90 (sets initial conditions, defines the vertical grid,
   defines and reads/prints parameters for this test case).
3. An example input parameter file test_case.in.


**********************************************************************************************************************************
COMPILING
The compile options and their default values are given at the start of the Makefile.

The code has been tested for the gfortran and ifort compilers and the openmpi and intelmpi mpi libraries. gfortran/openmpi is the
recommended choice for portability, and because AMPI (see below) does not currently work with intel.

************************************************************
PARALLEL AND SERIAL VERSIONS
ARCH=mpi (default load balancing and checkpointing) or ARCH=ampi (AMPI load balancing and checkpointing) compiles
a parallel version.
ARCH=ser compiles a serial version, primarily for testing.

************************************************************
LOAD BALANCING
ARCH=mpi (default) rebalances the computational load statically at each checkpoint using a simple next fit algorithm.
ARCH=ampi uses charm++/AMPI to check the load and rebalance dynamically if necessary every irebalance time steps.

To use AMPI you first need to install the charm++/AMPI library as follows:
    git clone https://github.com/UIUC-PPL/charm
    cd charm/
    git fetch  &&  git checkout pieglobals-testing  (pieglobals are not included by default in version 7.0)

To build for a local machine with gfortran:

./build AMPI-only multicore-linux-x86_64 --with-production

To build for a multi-node machine, e.g. Compute Canada machine niagara, using gfortran (ifort does not work with AMPI):

module load NiaEnv/.2021a gcc openmpi autotools
./build AMPI-only mpi-linux-x86_64-smp -j16 --with-production


************************************************************
OPTIMIZATION
Default is OPTIM=2. Setting OPTIM=0 adds the flag -g and other compiler-dependent checks.

************************************************************
COARSEST GRID
The coarsest grid is set by the compile option PARAM=param_Jn, where n = 4, 5, 6, 7, or 8 is the number of subdivisions
of the icosahedron. 

The maximum number of computational cores must be less than or equal to the number of domains, i.e. Ncore <= 10*4**DOMAIN_LEVEL.
Since DOMAIN_LEVEL = Jmin - (PATCH_LEVEL+1) larger Jmin or smaller PATCH_LEVEL allows more cores to be used.  For inhomogeneous
problems (i.e. unbalanced) it is best to set PATCH_LEVEL=2 and use a larger Jmin while for homogeneous (i.e. well-balanced)
problems it is more efficient to choose a larger PATCH_LEVEL. For example, if Jmin = 7 and PATCH_LEVEL = 4 then up to 160 cores
may be used, while if PATCH_LEVEL = 2 then up to 2560 cores may be used. These options are set in the files src/param_Jn.f90.

The finest allowable grid is set by the parameter max_level in the input file (e.g. test_case.in). The finest grid actually
used changes dynamically and depends on the solution and the tolerance parameter tol.

************************************************************
EXAMPLE
To compile test case Held_Suarez using mpi and ifort with J5 as the coarsest grid:

make TEST_CASE=Held_Suarez PARAM=param_J5 ARCH=mpi F90=ifort

!! Always do "make clean" when compiling a new test case and when you modify the test case !!

To compile test case jet using charm++/AMPI and gfortran with J5 as the coarsest grid:

make TEST_CASE=jet PARAM=param_J6 ARCH=ampi F90=gfortran

!! Note that AMPI does not work properly with ifort !!


**********************************************************************************************************************************
RUNNING

Initial steps (using test case Held_Suarez as an example).

1. cd to the working directory (mkdir if necessary).

2. Provide symbolic links to all files required for execution:

ln -s wavetrisk_hydrostatic/bin/Held_Suarez

Also include Held_Suarez.user.so and charmrun, in addition to executable Held_Suarez, for AMPI.

3. Provide a symbolic link to the Heikes and Randall optimized grids (if optimize_grid=HR_GRID):

ln -s path-to-grid_HR

The compressed Heikes and Randall grids J2, ... , J8 are provided in cmpressed format in data/grid_HR.7z.

4. Copy input parameters file and edit as appropriate:

cp wavetrisk_hydrostatic/test/Held_Suarez/test_case.in .

Note: the code runs significantly faster with mpirun than with srun when using the slurm scheduler.

If running with bathymetry data (e.g. incompressible tsunami test case) you need to provide a symbolic link to the ETOPO data:

ln -s ../extern/bathymetry/2arcminutes.smoothed bathymetry (to use 2 arcminute etopo topography data)

************************************************************
RUN EXAMPLES

1. With ARCH=mpi, to run test case Held_Suarez with input file HS.in on a local machine with shared memory 40 cores

mpirun -n 40 ./Held_Suarez HS.in 

2. With ARCH=ampi, to run test case jet with default input file test_case.in using charm++/AMPI on a local machine with 40 cores
using 160 virtual processors and "Refine load balancing":

./charmrun +p 60 ./jet_J6 test_case.in +vp 160 +balancer RefineLB +LBDebug 1 > log&

To restart from a saved checkpoint

1. With ARCH=mpi, set resume = N in the input file, where N = index of the checkpoint to restart from (resume = -1 is a fresh start).

2. With ARCH=ampi indicate the checkpoint in the command line:

./charmrun +p 60 ./jet_J6 test_case.in +vp 160 +balancer RefineLB +LBDebug 1 +restart directory_name

where directory_name is the directory containing the checkpoint to restart from.

3. Multi node run on Compute Canada machine niagara using AMPI on 2 nodes (each node has 2 sockets with 20 cores each).

Note that you need to launch 19 worker threads and 1 dedicated communication thread per socket, i.e. you must reserve
1 thread for communication on each socket.

./charmrun +p 76 ./drake_charm_J6 2layer.in +ppn 19 +vp 76 +pemap 1-19,21-39 +commap 0,20

p    = number of physical processors (limited by the number of available nodes and cores per node)
ppn  = number of threads per socket  (determined by node characteristics)
pemap  specifies which cores to use on each node
commap specifies which cores to use for communication threads 

vp   = number of virtual processors  (<= number of coarse scale domains 10*4**DOMAIN_LEVEL)

In this case, p = (2 nodes) * (19 threads  * 2 sockets) = 76. Note that ppn, pemap, and commap are specified per node,
so they don't change as you modify the number of nodes.

The ratio vp/p determines the degree of "virtualization" used for load balancing. Typically, vp/p = 4 to 16 is optimal.

**********************************************************************************************************************************
POST-PROCESSING DATA USING PARAVIEW AND MATLAB

The raw data files (e.g. Held_Suarez.1... and Held_Suarez.2...) must be converted to .vtk format for viewing with paraview:
 
cd post gfortran -o trisk2vtk trisk2vtk.f90 cd to-working-directory ln -s wavetrisk_hydrostatic/post/trisk2vtk .  trisk2vtk
Held_Suarez.1 primal 0 10 5 8 out (generates .vtk files for the saved hexagon grid fields 0 to 10 at scales j = 5:8 and saves in
files out...vtk) trisk2vtk Held_Suarez.2 dual 0 10 5 8 out (generates .vtk files for the saved triangle grid fields 0 to 10 at
scales j = 5:8 and saves in files out...vtk). Advice on using paraview for visualization is given in post/paraview_notes.

************************************************************
POST-PROCESS CHECKPOINTS FOR MATLAB PLOTTING OF 2-D PROJECTIONS ON LONGITUDE-LATITUDE PLANE

The m-file plot_2d.m provides various options for plotting 2-D projections of the data, zonal statistics and time history
statistics.  The zonal statistics can be plotted from in-line calculations (e.g. Held_Suarez_J4L18.3.tgz file saved during the
run) or from data post-processed from saved checkpoints (e.g. Held_Suarez_J4L18.4.tgz) using test/flat_projection_data.  plot_2d.m
plots time history data from the log file saved during the run (e.g. Held_Suarez_log).  (See also 2D PROJECTION below.)
Additional post-processing m-files are provided for specific test cases and applications.

************************************************************
SPHERICAL HARMONICS GLOBAL AND LOCAL SPECTRAL ANALYSIS

The test case spherical_harmonics provides tools for computing spherical harmonics energy spectra. The energy spectra may be
computed over the entire sphere, or over a local spherical cap region.  This test case uses the SHTOOLS package, which is
available at https://shtools.github.io/SHTOOLS/index.html.  The algorithms used in SHTOOLS are described in:

Wieczorek, M. A. and F. J. Simons 2007 Minimum-variance multitaper spectral estimation on the sphere, J. Fourier Anal. Appl., 13,
doi:10.1007/s00041-006-6904-1, 665-692.

Wieczorek, M. A. and Meschede, M. 2018 SHTools: Tools for Working with Spherical Harmonics.Geochemistry, Geophysics, Geosystems,
19(8), 2574-2592.

SHTOOLS requires the libraries lapack, blas and fftw3.

An example matlab m-file spherical_harmonic_analysis.m is also provided for visualizing the spectra produced by this test case.


**********************************************************************************************************************************
CODE DESCRIPTION

************************************************************
PROGNOSTIC VARIABLES

The variables are stored in a Float_Array (see below) sol, with components S_MASS, S_TEMP and S_VELO.  The code solves the
hydrostatic Boussinesq equations in compressible (i.e. atmosphere) or incompressible (i.e. ocean) form.

S_MASS contains mu = ref_density * dz (i.e. kinemetic/inert pseudo density).

S_TEMP contains the mass-weighted potential temperature mu * potential temperature in the compressible case or mu *
density/ref_density = density * dz (i.e. gravitational mass) in the incompressible case. The variable theta is defined to be
S_TEMP/S_MASS, so theta is the potential temperature in the compressible case, and theta is the normalized total density in the
incompressible case (i.e. buoyancy = grav_accel * (1 - theta)).

S_VELO contains the three velocity components U, V, W at edges RT, DG and UP respectively (see below).

************************************************************
PARAMETERS

The full list of parameters is given in shared.f90, together with their default values.

************************************************************
HORIZONTAL COORDINATES

Cartesian (x,y,z) coordinates are used with spherical geometry for lengths and areas on the sphere.  Since the equations are
written in vector invariant form, there are no explicit metric terms.

************************************************************
VERTICAL COORDINATES

The zero level of the vertical coordinate is at mean sea level and vertical levels are always indexed starting at the lowest
level.  The coordinate of the local topography is given by dom%topo%elts(id_i) (or by the test case function surf_geopot in the
case of analytical topography), which is positive for orography and negative for bathymetry.  For incompressible (i.e. ocean)
simulations the mean depth is defined to be negative, H0 = dom%topo%elts(id_i) < 0, and positive perturbations to the free surface
are positive.  The local total depth is H = sum_zlev ( sol(S_MASS,zlev)%data(d)%elts(id_i) ) / ref_density and the free surface
perturbation is therefore H + H0.

The vertical coordinates are Lagrangian (i.e. they move as material surfaces) with periodic remapping onto a target (e.g. initial
grid).  A collection of different conservative piecewise remapping routines are available.

************************************************************
SOLID BOUNDARIES

In ocean test cases solid lateral boundaries are modelled using Brinkman volume penalization (see Kevlahan, Dubos & Aechtner 2015,
Geosci Model Dev 8).  The permeability parameter is fixed equal to dt and the porosity parameter alpha is set by the user
(e.g. alpha = 0.01). The permeability friction term -1/eta u in the velocity equation is applied as a separate split step after the
main time step, as in Rassmussen, Cottet & Walther (J Comput Phys 230, 2011).

************************************************************
BAROTROPIC - BARCOCLINIC MODE SPLITTING

Incompressible (i.e. ocean) simulations can be run with 2D barotropic - 3D baroclinic mode splitting to avoid the stability
constraint of the fast external mode with speed c = sqrt(g H). Setting mode_split = .true. chooses mode splitting, which integrates
the barotropic component implicitly in time on the slow geostrophic time scale. The computation is stable with
CFL_barotropic = sqrt(g H) dt/dx < = 30, and often at larger values. The method is similar to the "implicit free surface"
option in MITgcm.  The splitting usese a semi-implicit theta-method for the barotropic mode, with parameters theta1 (for the
external pressure gradient) and theta2 for the barotropic flow divergence.  Theta1 = theta2 = 1 gives a backwards Euler (fully
implicit scheme), and theta1 > 0.5, theta2 > 0.5 is necessary for stability.  The recommended value is thata1 = theta2 = 0.8.

The elliptic equation for the free surface is solved using a simple adaptive multigrid solver. The coarsest scale is solved using
bicgstab, and this solution is then prolonged to the finer scales, which are solved using scheduled relaxation Jacobi iterations
(Adsuara et al J Comput Phys 332, 2017). Recommended relative tolerance for the (coarse scale) elliptic solver
tol_elliptic = 1e-9 to ensure mass conservation and sufficient accuracy. A larger relative tolerance tol_jacobi = 1e-3 to 1e-6
may be used for the finer scales. 

The associated elliptic equation is solved using a simple multigrid scheme on the adaptive grid. The baroclinic and barotropic
modes are coupled at each time step. The barotropic part computes an implicit free surface. The barotropic estimate of the
location of the free surface is reconciled with the baroclinic estimate using layer dilation (Bleck and Smith 1990,
J Geophys Res 95, 3273-3285). Layer dilation means that mass is not exactly conserved in each vertical layer, although it is
conserved over all vertical layers. Mode splitting is suitable when the user is not interested in the evolution of the
free surface, since it diffuses free surface motion. When the option remap = .false. it is assumed that the density remains
constant in each vertical layer, equal to the initial density distribution.  Otherwise, horizontal density gradients are included
with Ripa dynamics (e.g. Ripa 1993, Geophys Astrophys Fluid, 70). The drake test case also includes an option that relaxes the
layer densities to their initial values. This model is also called IL0 (Beron-Vera 2021, Rev Mex Fis 67), or a "thermal rotating 
shallow-water model".

The parameter level_fill can be set > min_level to make active all levels <= level_fill. This sometimes improves the efficiency
of the multigrid solver by reducing the size of of the coarsest grid where the elliptic equation is solved exactly.

************************************************************
VERTICAL DIFFUSION AND SURFACE FLUXES IN OCEAN MODEL (INCOMPRESSIBLE CASE)

Eddy diffusion of buoyancy and eddy viscosity for velocity is implemented using a flux-based implicit time integration scheme  
in the module vert_diffusion_mod (option vert_diffuse = .true.).  Wind stress, bottom friction and buoyancy fluxes are included 
via flux (Neumann) boundary conditions. A solar radiation model (as in NEMO) is activated by setting a non-zero value for the
solar radiation flux Q_sr. Two options are available: tke_diffuse = .true. uses a turbulent kinetic energy (TKE) closure scheme,
similar to that in NEMO, tke_diffuse = .false. uses a simple depth-based eddy-viscosity.

************************************************************
GRID STRUCTURE

The icosahedron is divided into a network of ten regular lozenge grids (with the exception of the POLES), each lozenge is then
divided into N_SUB_DOM = 4**DOMAIN_LEVEL regular sub-domains with the number of sub-domains on each processor given by
n_domain(rank+1).

There are two special nodes called POLEs. One connects the five lozenge vertices at the top of the network and the other connects
the five lozenge grid vertices at the bottom of the network. The network is oriented so the POLEs are at the geographic North and
South poles.

The coarsest level Jmin = DOMAIN_LEVEL + PATCH_LEVEL + 1 with PATCH_LEVEL>=2. The geometry of this coarsest level may be optimized
by reading in a Heikes & Randall (1995) (optimize_grid = HR_GRID) grid of resolution Jmin-1 or using the Xu (2006) smoothing
algorithm (optimize_grid = XU_GRID). If optmize_grid = NONE the coarsest grid is just the grid produced by bisecting the
icosahedron the Jmin times. The size of patches is 2**PATCH_LEVEL (e.g. if PATCH_LEVEL = 2 then the patches are 4x4).  The
larger PATCH_LEVEL is the fewer levels of tree structure must be traversed before reaching a uniform grid (which will in general
contain inactive nodes).  Each computation patch element is made up of one node for scalars and three edges for vectors
U, V, W (see also GRID ELEMENTS below).

************************************************************
2D PROJECTION ONTO A LONGITUDE-LATITUDE GRID

The module projection_mod includes routines for projecting data from the sphere to a uniform longitude-latitude grid.  These can
be used to save projections for post-processing, spherical harmonics, or for zonal averaging.  Note that these routines require
a "full" (i.e. non-adaptive grid) which can be either min_lev, or computed using call fill_up_grid_and_IWT (level_save).

************************************************************
BASIC GRID DATA TYPES

Type(Coord_Array) has components elts(:)%length (where elts is Type(Coord) array of x%y%z coordinates on the sphere for each grid
element and length is an integer giving the size of elts).

Type(Float_Array) has components elts(:)%length (where elts is a real array of variable values on each grid element on each
sub-domain and and length is an integer giving the size of elts). It is used for physical variables such as coriolis, divu, vort
etc.

Type(Int_Array) has components elts(:)%length (where elts is an integer array of element indices on each sub-domain and and length
is an integer giving the size of elts). It is used for masks, levels and parallel communication variables.

Type(Float_Field) has components data(:)%bdry_uptodate%pos (where data is a Float_Array, bdry_uptodate is a logical and pos is an
integer) and is used for equation variables such as sol, trend, wav_coeff, sca_coeff.

Type(Domain) has many components defining the grid and is the size of the total number of sub-domains on each processor
n_domain(rank+1).  It is used for the variable grid(:).

Various other dynamical data types and subroutines for allocating them are defined in dyn_array.f90.


************************************************************
GRID ELEMENTS

The triangular primal grid element (one hexagonal node H, three edges UP, DG, RT and two triangular nodes UPLT, LORT) is

             ------------ 
             \           / \ 
              \  UPLT   /   \ 
               \       /     \
               UP     DG      \   
                 \   /   LORT  \
                  \ /           \
                   H ------RT---- 


The hexagonal dual grid element: one hexagonal node H, three adjacent edges UP, DG, RT and two triangular nodes O
(rotated clockwise 30 degrees wrt primal grid above) is



             -----UP---- UPLT 
           /               \ 
          /                 \ 
         /                  DG
        /                     \   
       /                       \
                   H           LORT
       \                       /
        \                     /
         \                   RT
          \                 / 
           \               /
             -------------


Patch neighour directions (based on regular coordinates i,j).
Note that within a patch similar notation is used for shifts in i and j (e.g. shift i-1, j+1 is denoted idNW).
                   
 ------------- ------------- ------------- 
\              \             \            \
 \              \             \            \
  \              \             \            \
   \  NORTHWEST   \    NORTH    \ NORTHEAST  \  
    \              \             \            \
     \              \             \            \
       -------------  ------------- ------------- 
       \              \             \            \
        \              \             \            \
         \              \             \            \  
          \     WEST     \      0      \    EAST    \ 
           \              \             \            \
            \              \             \            \
              -------------  ------------- ------------- 
              \              \             \            \
               \              \             \            \ 
                \              \             \            \
                 \   SOUTHWEST  \  SOUTH      \  SOUTHEAST \
                  \              \             \            \
                   \              \             \            \
                     -------------   ------------ ------------ 

************************************************************
INDEXING OF GRID ELEMENTS AND NEIGHBOURS

Quantities (e.g. mass, velocities) are all stored in a single array, whose elements are organized in patches.  Each patch has
regular array coordinates (i,j).

Patch offset array offs(N_BDRY+1) contains the starting index in the single array for the current patch as offs(0). The starting
indices for neighbouring patches are given by offs(NORTH) etc.

Patch dimension array dims(2, N_BDRY+1) gives the dimensions of the current patch as dims(2,0) and neighbouring patches as dims(2,
NORTH) etc.

Function id = idx(i,j,offs,dims) returns the element index for the hexagon node with coordinates (i,j) on the patch selected by
offs with dimensions dim.

The components of the grid elements are then found as:

 %elts(id+1) - the one grid element hexagon node (e.g. masses) elts(EDGE*id+e+1) - the three grid element edges RT, DG, UP, where
 %e = 0,1,2 (e.g. velocities, fluxes) elts(TRIAG*id+t+1) - the two grid element triangles LORT, UPLT, where t = 0,1
 %(e.g. circulation)

Wavelet coefficients are stored at the SAME nodes/edges as the nodes/edges they characterize.

************************************************************
DATA OUTPUT

Data is written for plotting by routines io.f90/write_primal and io.f90/write_dual.  The full state of the simulation is saved in
io.f90/dump_adapt_mpi and read in again by io.f90/load_adapt_mpi.

************************************************************
CALCULATIONS ON ADAPTED GRID

By default fields are calculated and operators are applied on the ENTIRE grid (including at nodes where the result can be obtained
by restriction indicated by mask=12 and adjacent zone nodes indicated by mask=8) and the results are then over-written by correct
values.  (Note that the solution in the adjacent zone at fine scale j+1 are found from values at coarse scale j so the values
calculated at scale j+1 are not actually used.)

This means that some operations on the entire grid could produce intermediate overflows, inf/NaN, or invalid indices due to
incorrect values at these nodes or their neighbours. Functions and subroutines should take this into account.  Similarly,
circulation, vorticity and qe (potential vorticity are first computed incorrectly at pentagon points in step1 (or cal_vort) and
then corrected in post_step1 (or post_vort).
